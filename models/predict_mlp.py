#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MLP ê¸°ë°˜ ë‹¤ì¤‘ ë ˆì´ë¸” ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ - ì˜ˆì¸¡/í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

- train_mlp.pyì—ì„œ í•™ìŠµ ë° ì €ì¥í•œ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ë°ì´í„° ë¶„í•  ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™€ì„œ
  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥ í‰ê°€ ë° ìƒˆ ë°ì´í„° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
"""

import os
import json
import numpy as np
import pandas as pd
from datetime import datetime
import pickle

import torch
import torch.nn as nn
from sklearn.metrics import classification_report, multilabel_confusion_matrix

# =====================================
# 0. ì„¤ì • (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼ ë””ë ‰í† ë¦¬ êµ¬ì¡° ê°€ì •)
# =====================================
OUTPUT_DIR_MODELS = "models"
OUTPUT_DIR_RESULTS = "results/mlp"

MODEL_PATH = os.path.join(OUTPUT_DIR_MODELS, "mlp_classifier.pth")
SCALER_PATH = os.path.join(OUTPUT_DIR_MODELS, "scaler_mlp.pkl")
SPLIT_PATH = os.path.join(OUTPUT_DIR_MODELS, "data_split.npz")

# ë ˆì´ë¸” ë§¤í•‘ (í•™ìŠµê³¼ ë™ì¼)
LABEL_MAPPING = {
    "fluctuating": [1, 0],
    "misalignment": [0, 1],
    "normal": [0, 0],
}

ANOMALY_LABELS = ["fluctuating", "misalignment"]  # ì¶œë ¥ ë²¡í„°ì˜ ë‘ ì¶•


# =====================================
# 1. MLP ëª¨ë¸ ì •ì˜ (train_mlp.pyì™€ ë™ì¼í•´ì•¼ í•¨)
# =====================================
class MLPClassifier(nn.Module):
    def __init__(self, input_size, hidden_sizes=(64, 32), output_size=2):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_sizes[0])
        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.fc3 = nn.Linear(hidden_sizes[1], output_size)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))  # ì¶œë ¥ì€ [0, 1] í™•ë¥  ë²¡í„°
        return x


# =====================================
# 2. ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ë°ì´í„° ë¡œë“œ ìœ í‹¸
# =====================================
def load_model_scaler_and_split():
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"Model not found: {MODEL_PATH}")
    if not os.path.exists(SCALER_PATH):
        raise FileNotFoundError(f"Scaler not found: {SCALER_PATH}")
    if not os.path.exists(SPLIT_PATH):
        raise FileNotFoundError(f"Data split not found: {SPLIT_PATH}")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(MODEL_PATH, map_location="cpu")
    input_size = checkpoint["input_size"]
    hidden_sizes = checkpoint.get("hidden_sizes", (64, 32))
    output_size = checkpoint.get("output_size", 2)

    # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = MLPClassifier(
        input_size=input_size,
        hidden_sizes=hidden_sizes,
        output_size=output_size,
    )
    model.load_state_dict(checkpoint["model_state_dict"])

    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    with open(SCALER_PATH, "rb") as f:
        scaler = pickle.load(f)

    # train/test ë¶„í•  ë°ì´í„° ë¡œë“œ
    split = np.load(SPLIT_PATH)
    X_train = split["X_train"]
    X_test = split["X_test"]
    y_train = split["y_train"]
    y_test = split["y_test"]

    return model, scaler, X_train, X_test, y_train, y_test, checkpoint


# =====================================
# 3. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
# =====================================
def evaluate_on_test_set():
    print("\n" + "=" * 60)
    print("ğŸ“‚ Loading model / scaler / data split")
    print("=" * 60)

    model, scaler, X_train, X_test, y_train, y_test, checkpoint = load_model_scaler_and_split()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    print(f"âœ… Using device: {device}")
    print(f"âœ… X_train shape: {X_train.shape}")
    print(f"âœ… X_test shape: {X_test.shape}")
    print(f"âœ… y_train shape: {y_train.shape}")
    print(f"âœ… y_test shape: {y_test.shape}")

    # Tensor ë³€í™˜
    X_test_tensor = torch.FloatTensor(X_test).to(device)

    # ì˜ˆì¸¡
    with torch.no_grad():
        y_pred_proba = model(X_test_tensor).cpu().numpy()   # (N, 2), 0~1
        y_pred = (y_pred_proba > 0.5).astype(int)           # threshold 0.5

    print("\n" + "=" * 60)
    print("ğŸ“ˆ Classification Report (per-label, multi-label)")
    print("=" * 60)
    # 2ê°œì˜ ì¶•: fluctuating / misalignment
    print(
        classification_report(
            y_test,
            y_pred,
            target_names=ANOMALY_LABELS,
            digits=4,
            zero_division=0,
        )
    )

    # ì „ì²´ ë²¡í„° ì¼ì¹˜ ì •í™•ë„ (ì •ìƒ [0,0] í¬í•¨)
    vector_acc = float(np.mean(np.all(y_test == y_pred, axis=1)))
    print(f"\nâœ… Overall vector accuracy (including normal [0,0]): {vector_acc:.4f}")

    # ê° ë ˆì´ë¸”(ì¶•) ë³„ confusion matrix ë° ë©”íŠ¸ë¦­ ì €ì¥
    print("\n" + "=" * 60)
    print("ğŸ“Š Per-label metrics & confusion matrix")
    print("=" * 60)

    cm_all = multilabel_confusion_matrix(y_test, y_pred)

    os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)

    for i, label_name in enumerate(ANOMALY_LABELS):
        cm = cm_all[i]
        tn, fp, fn, tp = cm.ravel()

        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = (
            2 * (precision * recall) / (precision + recall)
            if (precision + recall) > 0
            else 0.0
        )

        metrics = {
            "model_type": "MLP Classifier (Multi-Label, 2-dim)",
            "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "label": label_name,
            "data_split": {
                "train_samples": int(len(X_train)),
                "test_samples": int(len(X_test)),
            },
            "metrics": {
                "vector_accuracy_overall": vector_acc,
                "accuracy": float(accuracy),
                "precision": float(precision),
                "recall": float(recall),
                "f1_score": float(f1),
            },
            "confusion_matrix": {
                "true_negative": int(tn),
                "false_positive": int(fp),
                "false_negative": int(fn),
                "true_positive": int(tp),
            },
        }

        metrics_path = os.path.join(OUTPUT_DIR_RESULTS, f"mlp_metrics_{label_name}.json")
        with open(metrics_path, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)

        print(f"[{label_name}]")
        print(f"  TN={tn}, FP={fp}, FN={fn}, TP={tp}")
        print(f"  accuracy = {accuracy:.4f}")
        print(f"  precision = {precision:.4f}")
        print(f"  recall = {recall:.4f}")
        print(f"  f1 = {f1:.4f}")
        print(f"  â†’ saved: {metrics_path}")

    print("\n" + "=" * 60)
    print("âœ… Test evaluation completed")
    print("=" * 60)

    # ì˜ˆì¸¡ í™•ë¥ /ë²¡í„° ìì²´ë„ npzë¡œ ë¤í”„ (í•„ìš”í•˜ë©´ anomaly-vector ë¶„ì„ì— ì‚¬ìš©)
    pred_dump_path = os.path.join(OUTPUT_DIR_RESULTS, "mlp_test_predictions.npz")
    np.savez(
        pred_dump_path,
        y_test=y_test,
        y_pred=y_pred,
        y_pred_proba=y_pred_proba,
    )
    print(f"\nğŸ“ Saved raw predictions: {pred_dump_path}")


# =====================================
# 4. ìƒˆ CSVì— ëŒ€í•œ ì˜ˆì¸¡ ìœ í‹¸
# =====================================
def predict_from_csv(csv_path: str, model=None, scaler=None):
    """
    1118_features*_cleaned.csv í˜•íƒœì˜ íŠ¹ì§• ë²¡í„° CSVë¥¼ ì…ë ¥ë°›ì•„
    windowë³„ ì´ìƒë„ ë²¡í„° / ì˜ˆì¸¡ ë¼ë²¨ì„ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.

    - csv_path: window_id, start_time, end_time, feature1, feature2, ... êµ¬ì¡° ê°€ì •
    - model, scalerë¥¼ ì¸ìë¡œ ë„˜ê¸°ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œí•¨
    """
    if model is None or scaler is None:
        model, scaler, _, _, _, _, _ = load_model_scaler_and_split()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    df = pd.read_csv(csv_path)

    if df.shape[1] <= 3:
        raise ValueError("CSVì— feature ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì• 3ê°œ ë©”íƒ€ ì»¬ëŸ¼ ì´í›„ì— íŠ¹ì§•ì´ ìˆì–´ì•¼ í•¨)")

    meta_cols = df.iloc[:, :3]
    X_raw = df.iloc[:, 3:].values

    # í•™ìŠµ ë•Œì™€ ë™ì¼í•œ scaler ì‚¬ìš©
    X_scaled = scaler.transform(X_raw)

    X_tensor = torch.FloatTensor(X_scaled).to(device)
    with torch.no_grad():
        y_proba = model(X_tensor).cpu().numpy()
        y_bin = (y_proba > 0.5).astype(int)

    # L2 norm (ë³µí•© ì´ìƒë„)
    vector_mag = np.linalg.norm(y_proba, axis=1)

    # ê°„ë‹¨í•œ ë¼ë²¨ ë¬¸ìì—´ë¡œ ë§¤í•‘
    label_strs = []
    for vec in y_bin:
        if np.array_equal(vec, [0, 0]):
            label_strs.append("normal")
        elif np.array_equal(vec, [1, 0]):
            label_strs.append("fluctuating")
        elif np.array_equal(vec, [0, 1]):
            label_strs.append("misalignment")
        else:
            # [1,1] ë“± ë³µí•© ì´ìƒ
            label_strs.append("composite")

    # ê²°ê³¼ DataFrame êµ¬ì„±
    result_df = meta_cols.copy()
    result_df["prob_fluctuating"] = y_proba[:, 0]
    result_df["prob_misalignment"] = y_proba[:, 1]
    result_df["anomaly_vector_norm"] = vector_mag
    result_df["pred_label"] = label_strs

    # ê¸°ë³¸ ì €ì¥ ê²½ë¡œ (ë¬´ì¡°ê±´ results/mlp ì•„ë˜ì— ì €ì¥)
    os.makedirs(OUTPUT_DIR_RESULTS, exist_ok=True)
    
    filename = os.path.basename(csv_path)            # ì˜ˆ: 1118_features_cleaned.csv
    base, ext = os.path.splitext(filename)
    out_path = os.path.join(OUTPUT_DIR_RESULTS, base + "_predicted" + ext)
    
    result_df.to_csv(out_path, index=False)
    print(f"\n   â†’ Saved with predictions: {out_path}")

    return result_df


# =====================================
# 5. main
# =====================================
if __name__ == "__main__":
    # 1) ë¨¼ì € í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
    # evaluate_on_test_set()

    # 2) í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ë¶€ë¶„ ìˆ˜ì •í•´ì„œ ì„ì˜ CSVì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
    # ì˜ˆì‹œ:
    # predict_from_csv("data/processed/1118_features_cleaned.csv")
    predict_from_csv("data/processed/1118_features_fluctuating_yellow_cleaned.csv")
    predict_from_csv("data/processed/1118_features_misalignment_yellow_cleaned.csv")